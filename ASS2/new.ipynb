{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 120.373, Accuracy: 0.000\n",
      "Output Layer\n",
      "dinputs [[-0.21610425 -0.40733397 -0.42389001]\n",
      " [-0.11423906 -0.21532871 -0.22408071]\n",
      " [-0.72506607 -1.36667388 -1.42222217]\n",
      " [-0.47047368 -0.8867938  -0.92283742]\n",
      " [-0.67409646 -1.2706015  -1.32224494]\n",
      " [-0.52168204 -0.98331622 -1.02328298]\n",
      " [-0.31796136 -0.59932398 -0.62368343]\n",
      " [-0.9288487  -1.75078289 -1.82194323]] (8, 3)\n",
      "mask [[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]] (8, 3)\n",
      "dinputs [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]] (8, 3)\n",
      "Hidden Layer\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (8,8) (8,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 211\u001b[0m\n\u001b[0;32m    209\u001b[0m model \u001b[38;5;241m=\u001b[39m Neural_Network(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], n_hidden_layers, nodes_in_hidden_layers, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m    213\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn[7], line 198\u001b[0m, in \u001b[0;36mNeural_Network.train\u001b[1;34m(self, X, y, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    194\u001b[0m accuracy_value \u001b[38;5;241m=\u001b[39m accuracy\u001b[38;5;241m.\u001b[39mcalculate(y_pred, y_batch)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_value\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m )\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 174\u001b[0m, in \u001b[0;36mNeural_Network.backward\u001b[1;34m(self, y_pred, y_true, learning_rate)\u001b[0m\n\u001b[0;32m    172\u001b[0m         layer\u001b[38;5;241m.\u001b[39mbackward(output_error, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m         \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     output_error \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mactivation\u001b[38;5;241m.\u001b[39mdinputs\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n",
      "Cell \u001b[1;32mIn[7], line 61\u001b[0m, in \u001b[0;36mLayer.backward\u001b[1;34m(self, dvalues, output_layer)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdbiases \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dvalues, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# print(\"dbiases\", self.dbiases, self.dbiases.shape)\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdinputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 91\u001b[0m, in \u001b[0;36mSigmoid.backward\u001b[1;34m(self, dvalues)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dvalues):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdinputs \u001b[38;5;241m=\u001b[39m \u001b[43mdvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (8,8) (8,3) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_table(\"./data/abalone.data\", delimiter=\",\", header=None)\n",
    "\n",
    "# Convert categorical data to numerical\n",
    "data[0] = pd.factorize(data[0])[0]\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "# Split the data into batches\n",
    "\n",
    "\n",
    "# Define Layer class\n",
    "class Layer:\n",
    "    def __init__(self, n_inputs, n_neurons, type = 0):\n",
    "        self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        # self.weights = np.random.randint(1, 4, (n_inputs, n_neurons))\n",
    "        self.biases = np.random.randn(1, n_neurons)\n",
    "        if type == 0:\n",
    "            self.activation = Sigmoid()\n",
    "        else:\n",
    "            self.activation = ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.activation.forward(self.output)\n",
    "\n",
    "    def backward(self, dvalues , output_layer = 0):\n",
    "        # print(len(dvalues.shape))\n",
    "        if len(dvalues.shape) == 1:\n",
    "            dvalues = dvalues.reshape((dvalues.shape[0],1))\n",
    "\n",
    "        # print(dvalues.shape)[0]\n",
    "        # print(\"dvalues\", dvalues, dvalues.shape)\n",
    "        if output_layer:\n",
    "            print(\"Output Layer\")\n",
    "        else:\n",
    "            print(\"Hidden Layer\")\n",
    "\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        # print(\"dinputs\", self.dinputs, self.dinputs.shape)\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        # print(\"inputs\", self.inputs, self.inputs.shape)\n",
    "        # print(\"dweights\", self.dweights, self.dweights.shape)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # print(\"dbiases\", self.dbiases, self.dbiases.shape)\n",
    "        self.activation.backward(self.dinputs)\n",
    "        # print(\"dinputs\", self.activation.dinputs, self.activation.dinputs.shape)\n",
    "    def update(self, learning_rate):\n",
    "        print(type(learning_rate), type(self.dweights))\n",
    "        self.weights += learning_rate * self.dweights\n",
    "        self.biases += learning_rate * self.dbiases\n",
    "\n",
    "\n",
    "# Define ReLU activation function\n",
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        print(\"dinputs\", self.dinputs, self.dinputs.shape)\n",
    "        mask = self.dinputs <= 0\n",
    "        print(\"mask\", mask, mask.shape)\n",
    "        self.dinputs[self.dinputs <= 0] = 0\n",
    "        print(\"dinputs\", self.dinputs, self.dinputs.shape)\n",
    "        \n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # self.output = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        batch_loss = np.mean(sample_losses)\n",
    "        return batch_loss\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return np.mean((y_true - y_pred) ** 2, axis=0)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Define Accuracy metric\n",
    "class Accuracy:\n",
    "    def calculate(self, predictions, y):\n",
    "        comparisons = self.compare(predictions, y)\n",
    "        accuracy = np.mean(comparisons)\n",
    "        return accuracy\n",
    "    \n",
    "    def compare(self, predictions, y):\n",
    "        return predictions == y\n",
    "\n",
    "\n",
    "class Neural_Network:\n",
    "    def __init__(\n",
    "        self, n_inputs_attr, n_hidden_layers, nodes_in_hidden_layers, n_outputs\n",
    "    ):\n",
    "        # using Layer class to create layers\n",
    "        # nodes_in_hidden_layers is a list containing the number of nodes in each hidden layer\n",
    "        self.layers = []\n",
    "        for i in range(n_hidden_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(Layer(n_inputs_attr, nodes_in_hidden_layers[i]))\n",
    "            else:\n",
    "                self.layers.append(\n",
    "                    Layer(nodes_in_hidden_layers[i - 1], nodes_in_hidden_layers[i])\n",
    "                )\n",
    "        if n_hidden_layers > 0:\n",
    "            self.layers.append(Layer(nodes_in_hidden_layers[-1], n_outputs, 1))\n",
    "        else:\n",
    "            self.layers.append(Layer(n_inputs_attr, n_outputs, 1))\n",
    "\n",
    "    def print_layer(self, layer):\n",
    "        print(\"Layer Inputs\")\n",
    "        print(layer.inputs, layer.inputs.shape)\n",
    "        print(\"Layer Weights\")\n",
    "        print(layer.weights, layer.weights.shape)\n",
    "        print(\"Layer Biases\")\n",
    "        print(layer.biases, layer.biases.shape)\n",
    "        print(\"Layer Output\")\n",
    "        print(layer.output, layer.output.shape)\n",
    "        print(\"Layer Activation Output\")\n",
    "        print(layer.activation.output, layer.activation.output.shape)\n",
    "\n",
    "    def print_layers(self):\n",
    "        for layer in self.layers:\n",
    "            # if layer an object of Layer class\n",
    "            print(\"LAYER\")\n",
    "            if isinstance(layer, Layer):\n",
    "                self.print_layer(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            layer.forward(X)\n",
    "            X = layer.activation.output\n",
    "        return X\n",
    "\n",
    "    def backward(self, y_pred, y_true, learning_rate):\n",
    "        output_error = y_true - y_pred.flatten()\n",
    "        for layer in reversed(self.layers):\n",
    "            # print(\"LAYER\", self.layers.index(layer))\n",
    "            # self.print_layer(layer)\n",
    "            if layer == self.layers[-1]:\n",
    "                layer.backward(output_error, 1)\n",
    "            else:\n",
    "                layer.backward(output_error)\n",
    "            output_error = layer.activation.dinputs\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        batch_size = 8\n",
    "        X_train_batches = [\n",
    "            X[i : i + batch_size] for i in range(0, len(X), batch_size)\n",
    "        ]\n",
    "        y_train_batches = [\n",
    "            y[i : i + batch_size] for i in range(0, len(y), batch_size)\n",
    "        ]\n",
    "        loss = MeanSquaredError()\n",
    "        accuracy = Accuracy()\n",
    "        for epoch in range(epochs):\n",
    "            for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
    "                y_pred = self.forward(X_batch)\n",
    "                # print(\"y_pred, y_baytch\", y_pred, y_batch)\n",
    "                loss_value = loss.calculate(y_pred, y_batch)\n",
    "                accuracy_value = accuracy.calculate(y_pred, y_batch)\n",
    "                print(\n",
    "                    f\"Epoch: {epoch}, Loss: {loss_value:.3f}, Accuracy: {accuracy_value:.3f}\"\n",
    "                )\n",
    "                self.backward(y_pred, y_batch, learning_rate)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "config = [[0, [0]], [1, [32]], [1, [64]], [1, [3]]]\n",
    "# for config in configurations:\n",
    "n_hidden_layers = config[3][0]\n",
    "nodes_in_hidden_layers = config[3][1]\n",
    "# Create a new model with the given configuration\n",
    "model = Neural_Network(X_train.shape[1], n_hidden_layers, nodes_in_hidden_layers, 1)\n",
    "# Train the model\n",
    "model.train(X_train, y_train, 100, 0.001)\n",
    "# Test the model\n",
    "predictions = model.predict(X_test)\n",
    "# store predictions and actual values in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Neural_Network(X_train.shape[1], n_hidden_layers, nodes_in_hidden_layers, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10766408736063734\n"
     ]
    }
   ],
   "source": [
    "# neural_net.backward(y_pred, y_true, 0.01)\n",
    "# round the predictions \n",
    "predictions = np.round(predictions)\n",
    "a = Accuracy()\n",
    "print(a.calculate(predictions, y_test))\n",
    "combined_values = np.hstack((predictions.reshape(-1, 1), y_test.reshape(-1, 1)))\n",
    "\n",
    "# Save the combined values to a text file\n",
    "with open(f\"output_{n_hidden_layers}_{nodes_in_hidden_layers}.txt\", \"w\") as file:\n",
    "    # save loss \n",
    "    # file.write(f\"Loss: {loss_value:.3f}\\n\")\n",
    "    for row in combined_values:\n",
    "        file.write(f\"{row[0]} {row[1]}\\n\")\n",
    "# neural_net.print_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
